{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# The Misadventures of L2 Norm Triton Kernel\n",
    "\n",
    "This notebook details the iterative process, challenges, and solutions encountered while implementing a GPU-accelerated Euclidean norm (`nrm2`) routine using Triton kernels. This notebook illustrates common pitfalls in parallel programming and the importance of understanding how to ensure numerical correctness on GPUs.\n",
    "\n",
    "The goal was to efficiently calculate $||x||_2 = \\sqrt{\\sum_{i=1}^{N} x_i^2}$ for a given input vector $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "Before we begin, ensure you have the necessary libraries installed and a CUDA-enabled GPU available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected accelerator: cuda\n",
      "\n",
      "Input tensor size: 98432 elements\n",
      "First 10 elements of input tensor: tensor([-1.3399,  0.9823, -0.1188,  0.0460, -0.1024,  1.7807, -0.6536, -0.7665,\n",
      "        -0.2846, -0.3786], device='cuda:0')\n",
      "Using fixed BLOCK_SIZE for non-autotuned kernels: 256\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# check for accelerator\n",
    "if not torch.accelerator.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "accelerator = torch.accelerator.current_accelerator()\n",
    "print(f\"Detected accelerator: {accelerator}\")\n",
    "\n",
    "# input tensor\n",
    "SIZE = 98432\n",
    "x_torch = torch.randn(SIZE, device=accelerator, dtype=torch.float32)\n",
    "\n",
    "# fixed block size for demonstration (only autotuning in atomic add example)\n",
    "BLOCK_SIZE = 256\n",
    "\n",
    "print(f\"\\nInput tensor size: {SIZE} elements\")\n",
    "print(f\"First 10 elements of input tensor: {x_torch[:10]}\")\n",
    "print(f\"Using fixed BLOCK_SIZE for non-autotuned kernels: {BLOCK_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Challenge: Ensuring Correct Parallel Summation for L2 Norm\n",
    "\n",
    "The `nrm2` calculation requires squaring each element, summing all squared elements, and finally taking the square root. The most complex part from a GPU programming perspective is the highly parallel summation of potentially millions of squared values into a single scalar result, known as a **global reduction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: Single-Kernel Approach\n",
    "\n",
    "My initial strategy aimed for a single Triton kernel to perform the entire squaring and summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Misadventure: `tl.sum()` for Global Reduction\n",
    "\n",
    "**Concept:** Leverage Triton's `tl.sum()` directly on the squared elements, hoping it performs a complete global reduction across all threads and blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel output: tensor([16.3524], device='cuda:0') (nrm2 of only one block because of misadventure)\n",
      "Correct output: 313.6213073730469\n",
      "Number of blocks:  385\n",
      "Lesson: `tl.sum()` is a local reduction primitive, not a global one across all blocks, so this does not output the desired L2 norm.\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def global_sum_misadventure(\n",
    "    x_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    # sum of squares of the elements in the block\n",
    "    partial = tl.sum(x * x) \n",
    "\n",
    "    # THE MISADVENTURE: tl.sum() here operates only on the 'squares' \n",
    "    # loaded by THIS thread block. It DOES NOT sum across all blocks \n",
    "    # in the grid. If you launch many blocks, each calculates its own \n",
    "    # block_sum_of_squares, but they are not aggregated into a single global result.\n",
    "    total = tl.sum(partial[None], axis=0) \n",
    "\n",
    "    # sqrt only one block's sum\n",
    "    output = tl.sqrt(total) \n",
    "    tl.store(output_ptr, output) \n",
    "\n",
    "\n",
    "# test to show output is not a single global sum\n",
    "num_blocks = triton.cdiv(SIZE, BLOCK_SIZE)\n",
    "results = torch.zeros(1, device=accelerator, dtype=torch.float32)\n",
    "\n",
    "try:\n",
    "    global_sum_misadventure[(num_blocks,)](\n",
    "        x_torch, results, SIZE, BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    print(f\"Kernel output: {results} (nrm2 of only one block because of misadventure)\")\n",
    "    print(f\"Correct output: {torch.linalg.norm(x_torch)}\")\n",
    "    print(\"Number of blocks: \", num_blocks)\n",
    "    print(\"Lesson: `tl.sum()` is a local reduction primitive, not a global one across all blocks, so this does not output the desired L2 norm.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nAttempt 3.1 failed as expected for global reduction: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misadventure: `tl.atomic_add()` for Global Accumulation\n",
    "\n",
    "**Concept:** Shifted to using `tl.atomic_add()` within a single kernel. Each thread block computes its partial sum of squares and then atomically adds that partial sum to a single, shared global memory location that would eventually hold the total sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demonstrating `tl.atomic_add` accumulation issue with autotuner\n",
      "True sum of squares (torch.sum(x*x)): 98358.335938\n",
      "Autotuned result: 124440184.000000\n",
      "Autotuned result is 1265.17x the true sum of squares.\n",
      "The autotuner tested multiple configurations and accumulated ALL results!\n",
      "\n",
      "Lesson: While `atomic_add` ensures concurrent writes are safe, it does not reset state.\n",
      "Using it with the autotuner (which tests multiple configurations) leads to\n",
      "fundamentally incorrect, accumulated sums across ALL autotuner trials.\n"
     ]
    }
   ],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE': 64}),\n",
    "        triton.Config({'BLOCK_SIZE': 128}),\n",
    "        triton.Config({'BLOCK_SIZE': 256}),\n",
    "        triton.Config({'BLOCK_SIZE': 512}),\n",
    "    ],\n",
    "    key=['n_elements'],\n",
    ")\n",
    "@triton.jit\n",
    "def atomic_sum_misadventure(\n",
    "    x_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    \n",
    "    partial = tl.sum(x * x) \n",
    "    \n",
    "    # THE MISADVENTURE: The autotuner will test multiple configurations\n",
    "    # and accumulate results from ALL test runs into the same global_sum_ptr!\n",
    "    # This leads to massively inflated and numerically INCORRECT outputs.\n",
    "    # This highlights the danger of blindly trusting a system's correctness when it interacts \n",
    "    # with mutable global state across its multiple evaluation runs.\n",
    "    # (Referenced: https://github.com/triton-lang/triton/issues/6524)\n",
    "    tl.atomic_add(output_ptr, partial) \n",
    "\n",
    "\n",
    "# test\n",
    "print(\"\\nDemonstrating `tl.atomic_add` accumulation issue with autotuner\")\n",
    "\n",
    "# show the autotuned accumulation problem\n",
    "global_result = torch.zeros(1, device=accelerator, dtype=torch.float32)\n",
    "max_blocks = triton.cdiv(SIZE, 64)\n",
    "\n",
    "try:\n",
    "    atomic_sum_misadventure[(max_blocks,)](\n",
    "        x_torch, global_result, SIZE\n",
    "    )    \n",
    "    true_sum_of_squares = torch.sum(x_torch * x_torch).item()\n",
    "    print(f\"True sum of squares (torch.sum(x*x)): {true_sum_of_squares:.6f}\")\n",
    "    print(f\"Autotuned result: {global_result.item():.6f}\")\n",
    "    print(f\"Autotuned result is {global_result.item() / true_sum_of_squares:.2f}x the true sum of squares.\")\n",
    "    print(\"The autotuner tested multiple configurations and accumulated ALL results!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Autotuned version failed: {e}\")\n",
    "\n",
    "print(\"\\nLesson: While `atomic_add` ensures concurrent writes are safe, it does not reset state.\")\n",
    "print(\"Using it with the autotuner (which tests multiple configurations) leads to\")\n",
    "print(\"fundamentally incorrect, accumulated sums across ALL autotuner trials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: The Two-Kernel Approach\n",
    "\n",
    "To overcome the challenges of single-kernel global reductions, we adopted a multi-stage approach, splitting the problem into two distinct kernels.\n",
    "\n",
    "### `nrm2_partial` Kernel: Distributed Sum of Squares\n",
    "\n",
    "This kernel's role is to perform the element-wise squaring and sum those squares *within each thread block*. Each block then stores its partial sum to a **unique index** in a temporary global memory array. This avoids atomic contention in the first stage and ensures each block's result is isolated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_PARTIAL': 64}),\n",
    "        triton.Config({'BLOCK_SIZE_PARTIAL': 128}),\n",
    "        triton.Config({'BLOCK_SIZE_PARTIAL': 256}),\n",
    "        triton.Config({'BLOCK_SIZE_PARTIAL': 512}),\n",
    "        triton.Config({'BLOCK_SIZE_PARTIAL': 1024}),\n",
    "    ],\n",
    "    key=['n_elements'],\n",
    ")\n",
    "@triton.jit\n",
    "def nrm2_partial(\n",
    "    x_ptr,\n",
    "    partial_ptr,\n",
    "    n_elements,\n",
    "    BLOCK_SIZE_PARTIAL: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE_PARTIAL\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE_PARTIAL)\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    # sum within this block (intra-block reduction)\n",
    "    sum_of_squares = tl.sum(x * x) \n",
    "    \n",
    "    # no atomic needed here and safe with autotuning, as each program writes to a distinct memory address.\n",
    "    tl.store(partial_ptr + pid, sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nrm2_final` Kernel: Final Sequential Summation\n",
    "\n",
    "Instead of parallelizing the final sum with atomics, this kernel is launched with **only one thread block**. This single thread sequentially iterates through all `partial_sums` from the first kernel, accumulating them into a local accumulator variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def nrm2_final(\n",
    "    partial_ptr, \n",
    "    final_ptr,\n",
    "    n_partial, \n",
    "):    \n",
    "    total = tl.zeros((), dtype=tl.float32)\n",
    "    \n",
    "    # process partial sums sequentially\n",
    "    for i in range(n_partial):\n",
    "        partial_val = tl.load(partial_ptr + i)\n",
    "        total += partial_val\n",
    "        \n",
    "    tl.store(final_ptr, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nrm2` Function\n",
    "\n",
    "This Python function facilitates the two kernel launches and performs the final square root operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True L2 Norm (torch.linalg.norm): 313.621307\n",
      "Triton nrm2 custom kernel result: 313.621368\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "def nrm2(x: torch.Tensor):\n",
    "    output = torch.zeros((1,), device=x.device)\n",
    "    assert x.device == output.device, \"Input and output tensors must be on the same device.\"\n",
    "    n_elements = x.numel()\n",
    "\n",
    "    # calculate the maximum number of blocks that might be needed\n",
    "    min_block_size = 64  \n",
    "    max_partial_sums = triton.cdiv(n_elements, min_block_size)\n",
    "    partial_sums = torch.zeros(max_partial_sums, device=x.device, dtype=torch.float32)\n",
    "\n",
    "    # the autotuner will determine the optimal grid size based on the selected BLOCK_SIZE_PARTIAL\n",
    "    nrm2_partial[(max_partial_sums,)](\n",
    "        x,\n",
    "        partial_sums,\n",
    "        n_elements\n",
    "    )\n",
    "    \n",
    "    nrm2_final[(1,)](\n",
    "        partial_sums,\n",
    "        output,\n",
    "        max_partial_sums\n",
    "    )\n",
    "\n",
    "    # square root the sum of partial sums\n",
    "    result = torch.sqrt(output)\n",
    "    return result\n",
    "\n",
    "true_norm = torch.norm(x_torch, p=2).item()\n",
    "print(f\"True L2 Norm (torch.linalg.norm): {true_norm:.6f}\")\n",
    "\n",
    "result_nrm2_custom = nrm2(x_torch)\n",
    "print(f\"Triton nrm2 custom kernel result: {result_nrm2_custom.item():.6f}\")\n",
    "\n",
    "assert math.isclose(result_nrm2_custom.item(), true_norm, rel_tol=1e-5), \"Triton nrm2 result does not match torch.norm!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This kernel development process highlights valuable lessons in GPU kernel development specific to reduction patterns, focusing on achieving **numerical correctness** when designing parallel algorithms:\n",
    "\n",
    "* **Global Reduction Complexity:** Achieving correct global reductions often requires multi-stage approaches (e.g., block-wise partials, then a final aggregation). A single kernel trying to sum everything is difficult to get right.\n",
    "\n",
    "* **Atomics for Correctness, but Caution for Global State:** `tl.atomic_add` ensures correct concurrent updates to a single memory location. However, when used with repeated kernel calls in autotuning, it **accumulates results across calls, leading to fundamentally incorrect final sums**.\n",
    "\n",
    "* **Trusting Autotuner Correctness:** Do not implicitly trust the autotuner if your kernel modifies a global state that is not reset between its internal trials.\n",
    "\n",
    "* **Sequential Final Stages for Guaranteed Accuracy:** For the final aggregation step of a reduction, a single-threaded sequential sum can be a simple way to guarantee numerical correctness, but it comes at a cost on performance.\n",
    "\n",
    "* **Verification is Paramount:** Always verify custom kernel results against known correct implementations (like `torch.norm`) to ensure numerical accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
